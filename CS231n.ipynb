{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks for Visual Recognition\n",
    "http://cs231n.github.io"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nearest Neighbor Classifier\n",
    "\n",
    "** $L_1$ distance**\n",
    "\n",
    "$d_1(I_1,I_2) = \\sum_{p}|I_1^p-I_2^p|$\n",
    "\n",
    "where $I_1$, $I_2$ are vectors of two images being compared.\n",
    "\n",
    "**$L_2$ distance**\n",
    "\n",
    "$d_2(I_1,I_2) = \\sqrt{\\sum_{p}(I_1^p-I_2^p)^2}$\n",
    "\n",
    "**k-Nearest Neighbor Classifier**\n",
    "\n",
    "Motivation: instead of finding single closest image in the training set, we find the top k closest images and have them vote on the label of the test image.\n",
    "\n",
    "\n",
    "![alt text](./img/k-nearest.png \"Title\")\n",
    "\n",
    "### Pros and Cons of Nearest Neighbor Classifier\n",
    "**Pros**\n",
    "* Simple\n",
    "* Takes no time to train\n",
    "\n",
    "**Cons**\n",
    "* May work on low-dimensional data but distance over high-dimensional spaces can be very counter-intuitive\n",
    "* Images that are nearby each other are much more a function of the general color distribution of the images, or the type of background rather than their semantic identity.\n",
    "* The classifier must remember all the training data and store for future comparisons with the test data.\n",
    "* Classifying a test image is expensive since it requires a comparison to all training images.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation sets for Hyperparameter tuning\n",
    "\n",
    "**Example of hyperparameters:** the setting for k of k-nearest neighbor classifier.\n",
    "\n",
    "**Tuning hyperparameters:** split training test in two: a slightly smaller training set and what we call a validation set.\n",
    "\n",
    "In case where the size of training data might be small, people sometimes use more sophisticated techniques for hyperparameter tuning called **cross-validation**.\n",
    "\n",
    "![alt text](./img/k-fold.png \"Title\")\n",
    "\n",
    "Classic way is to split your training data randomly into train/val splits. As a rule of thumb, between 70-90% of your data usually goes to the train split. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The approach will have two major components: a **score function** that maps the raw data to class scores, and a **loss function** that quantifies the agreement between the predicted scores and the ground truth labels.\n",
    "\n",
    "For example, in CIFAR-10 we have a training set of N = 50,000 images, each with D = 32 x 32 x 3 = 3072 pixels, and K = 10, since there are 10 distinct classes (dog, cat, car, etc). \n",
    "\n",
    "### Linear Classifier\n",
    "\n",
    "$f(x_i, W, b) = Wx_i+b$\n",
    "\n",
    "where $x_i$ contains all pixels in the i-th image flattened into a single [3072 x 1] column. W is often called weights and is [10 x 3072] and b is called bias vector, with size of [10 x 1].\n",
    "\n",
    "**Example**\n",
    "\n",
    "![alt text](./img/classifier.png \"Title\")\n",
    "\n",
    "In the example shown above, the linear classifier compute the scores of a class as a weighted sum of all its pixel values across all 3 of its color channels. We assume the image only has 4 pixels and that we have 3 classes.\n",
    "\n",
    "\n",
    "**Bias Trick**\n",
    "\n",
    "We can combine two sets of parameters (the biases b and weights W) into a single matrix that holds both of them, in which we get:\n",
    "\n",
    "$f(x_i, W) = Wx_i$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function\n",
    "\n",
    "**Multiclass Support Vector Machine (SVM)**\n",
    "\n",
    "SVM loss is setup so that the correct class for each image to have a score higher than the incorrect class by some fixed amount margin $\\Delta$. Notice that it’s sometimes helpful to anthropomorphise the loss functions as we did above: The SVM “wants” a certain outcome in the sense that the outcome would yield a lower loss (which is good).\n",
    "\n",
    "$$L_i = \\sum_{j \\neq y_i}\\max (0, s_j-s_{yi}+\\Delta)$$\n",
    "\n",
    "where $y_i$ is the index of the correct class and $s_j = f(x_i,W)_j$ (the score for j-th class is the j-th element). \n",
    "\n",
    "We can also rewrite the loss function as:\n",
    "\n",
    "$$L_i = \\sum_{j \\neq y_i}\\max (0, w_j^T-w_{yi}^T+\\Delta)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization\n",
    "With the loss function presented above, a potential problem would be that there are a set of parameters W correctly classify every example. We want to encode some preference for certain set of weights W over others to remove this ambiguity.\n",
    "\n",
    "We can do so by extending the loss function with a regularization penalty R(w). The most common regularization penalty is L2 norm.\n",
    "\n",
    "$$R(W) = \\sum_{k} \\sum_{l} W_{k,l}^2$$\n",
    "\n",
    "The full multiclass SVM loss becomes:\n",
    "$$L = \\frac{1}{N} \\sum_i L_i + \\lambda R(W)$$\n",
    "\n",
    "or expanding this out in its full form:\n",
    "\n",
    "$$L = \\frac{1}{N} \\sum_i \\sum_{j \\neq y_i}max(0, w_j^T-w_{yi}^T+\\Delta) + \\lambda \\sum_{k} \\sum_{l} W_{k,l}^2$$\n",
    "\n",
    "L2 penalty leads to the appealing max margin property in SVM. The most appealing property is that penalizing large weights tends to improve generalization, because it means that no input dimension can have a very large influence on the scores by itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Practical Consideration\n",
    " \n",
    " **Setting Delta**\n",
    " \n",
    " It turns out that this hyperparameter can safely be set to $\\Delta$=1.0\n",
    " \n",
    " **Relation to Binary Support Vector Machine**\n",
    " \n",
    " $$L_i = Cmax(0,1,-y_iw^Tx_i)+R(W)$$\n",
    " \n",
    " where C is a hyperparameter and $y_i \\in \\{-1,1\\}$. This can be regard as a special case when there are only two classes in this SVM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax Classifier\n",
    "\n",
    "It turns out that SVM is one of two commonly seen classifiers. The other popular choice is Softmax classifier.\n",
    "\n",
    "$$L_i = -\\log (\\frac{e^{f_{yi}}}{\\sum_je^{f_j}})$$ or equivalently $$L_i = -f_{yi}+\\log \\sum_je^{f_j}$$\n",
    "\n",
    "It takes a vector of arbitrary real-valued scores and squashes it to vector values between 0 and one that sum to one.\n",
    "\n",
    "**Information theory view**\n",
    "\n",
    "The cross-entropy between a \"true\" distribution p and an estimated distributed q is defined as \n",
    "\n",
    "$$H(p,1)=-\\sum_xp(x)l\\log q(x)$$\n",
    "\n",
    "The softmax classifier is hence minimizing cross-entropy between the estimated class probabilities and the \"true\" distribution.\n",
    "\n",
    "Moreover, since the cross-entropy can be written in terms of entropy and the Kullback-Leibler divergence as $H(p,q)=H(p)+D_{KL}(p||q)$ and the entropy of the data function p is zero, this is also equivalent to minimizing the KL divergence between the two distributions.\n",
    "\n",
    "In other words, the cross-entropy objective wants the predicted distribution to have all of its mass on the correct answer.\n",
    "\n",
    "**Probabilistic interpretation**\n",
    "\n",
    "$$P(y_i|x_i:W)=\\frac{e^{f_{yi}}} {\\sum_j e^{f_j}}$$\n",
    "\n",
    "can be interpreted as the probability assigned to the correct label $y_i$, given the image $x_i$ and parameterized by W.\n",
    "\n",
    "**Practical issues: Numeric stability**\n",
    "\n",
    "When we are writing code for computing the Softmax function in practice, the intermediate term $e^{f_{yi}}$ and $\\sum_j e^{f_j}$ may be very large due to exponential. So it is important to use a normalization trick.\n",
    "\n",
    "$$\\frac{e^{f_{yi}}} {\\sum_j e^{f_j}} = \\frac{Ce^{f_{yi}}} {\\sum_j e^{f_j}}=\\frac{e^{f_{yi}+\\log C}} {\\sum_j e^{{f_j}+\\log C}}$$\n",
    "\n",
    "A common choice for C is to set $\\log C = -\\max_jf_j$.\n",
    "\n",
    "**Possibly confusing**\n",
    "\n",
    "To be precise, the SVM classifier uses the hinge loss, or also sometimes called the max-margin loss. The Softmax classifier uses cross-entropy loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM vs. Softmax\n",
    "\n",
    "In both cases we computer the same score vector f. The difference is in the interpretation of the scores in f: The SVM interprets these as class cores adn its loss function encourage the correct class to have a score higher by a margin than the other class scores.\n",
    "\n",
    "The Softmax classifier instead interprets the score as (unnormalized) log probabilities for each class and then encourages the (normalized) log probability of the correct class to be high. \n",
    "\n",
    "In practices, SVM and Softmax are usually comparable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization: Stochastic Gradient Descent\n",
    "\n",
    "Optimization: the process of finding the set of parameters W that minimize the loss function.\n",
    "\n",
    "Assume a simple dataset that contains three 1-dimensional points and three classes. The full SVM loss (without regulation) becomes:\n",
    "\n",
    "$$L_0 = \\max(0,w_1^Tx_0-w_0^Tx_0+1)+\\max(0,w_2^Tx_0-w_0^Tx_0+1)$$\n",
    "$$L_1 = \\max(0,w_0^Tx_1-w_1^Tx_1+1)+\\max(0,w_2^Tx_1-w_1^Tx_1+1)$$\n",
    "$$L_2 = \\max(0,w_0^Tx_2-w_2^Tx_2+1)+\\max(0,w_1^Tx_2-w_2^Tx_2+1)$$\n",
    "$$L=(L_0+L_1+L2)/3$$\n",
    "\n",
    "![alt text](./img/sum.png \"Title\")\n",
    "\n",
    "The SVM cost funciton is an example of a convex function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization Strategy\n",
    "\n",
    "**Random Search**\n",
    "\n",
    "Not quite a good idea..\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestloss = float(\"inf\") # Python assigns the highest possible float value\n",
    "for num in xrange(1000):\n",
    "  W = np.random.randn(10, 3073) * 0.0001 # generate random parameters\n",
    "  loss = L(X_train, Y_train, W) # get the loss over the entire training set\n",
    "  if loss < bestloss: # keep track of the best solution\n",
    "    bestloss = loss\n",
    "    bestW = W\n",
    "  print 'in attempt %d the loss was %f, best %f' % (num, loss, bestloss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Random Local Search**\n",
    "\n",
    "Extend one foot in a random direction and take a step only if it leads downhill. Concretely, we will start out with a random W, generate random perturbation $\\delta$W to it and if the loss at perturbed W+$\\delta$W is lower, we will perform an update\n",
    "\n",
    "**Following the Gradient**\n",
    "\n",
    "The gradient is a generalization of slope for functions that don't take a signle number but a vector of numbers. Gradient is just a vector of slopes (more commonly referred as derivatives) for each dimension in the input space.\n",
    "\n",
    "The mathematical expression for the derivative of a 1-D function with respect to its input is:\n",
    "\n",
    "$$\\frac{df(x)}{dx}=lim_{h \\to 0} \\frac{f(x+h)-f(x)}{h}$$\n",
    "\n",
    "When the functions of interest take a vector of numbers instead of a single number, we call the derivatives partial derivatives and the gradient is simply the vector or partial derivatives in each dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
