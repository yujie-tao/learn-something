{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks for Visual Recognition\n",
    "http://cs231n.github.io"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nearest Neighbor Classifier\n",
    "\n",
    "** $L_1$ distance**\n",
    "\n",
    "$d_1(I_1,I_2) = \\sum_{p}|I_1^p-I_2^p|$\n",
    "\n",
    "where $I_1$, $I_2$ are vectors of two images being compared.\n",
    "\n",
    "**$L_2$ distance**\n",
    "\n",
    "$d_2(I_1,I_2) = \\sqrt{\\sum_{p}(I_1^p-I_2^p)^2}$\n",
    "\n",
    "**k-Nearest Neighbor Classifier**\n",
    "\n",
    "Motivation: instead of finding single closest image in the training set, we find the top k closest images and have them vote on the label of the test image.\n",
    "\n",
    "\n",
    "![alt text](./img/k-nearest.png \"Title\")\n",
    "\n",
    "### Pros and Cons of Nearest Neighbor Classifier\n",
    "**Pros**\n",
    "* Simple\n",
    "* Takes no time to train\n",
    "\n",
    "**Cons**\n",
    "* May work on low-dimensional data but distance over high-dimensional spaces can be very counter-intuitive\n",
    "* Images that are nearby each other are much more a function of the general color distribution of the images, or the type of background rather than their semantic identity.\n",
    "* The classifier must remember all the training data and store for future comparisons with the test data.\n",
    "* Classifying a test image is expensive since it requires a comparison to all training images.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation sets for Hyperparameter tuning\n",
    "\n",
    "**Example of hyperparameters:** the setting for k of k-nearest neighbor classifier.\n",
    "\n",
    "**Tuning hyperparameters:** split training test in two: a slightly smaller training set and what we call a validation set.\n",
    "\n",
    "In case where the size of training data might be small, people sometimes use more sophisticated techniques for hyperparameter tuning called **cross-validation**.\n",
    "\n",
    "![alt text](./img/k-fold.png \"Title\")\n",
    "\n",
    "Classic way is to split your training data randomly into train/val splits. As a rule of thumb, between 70-90% of your data usually goes to the train split. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The approach will have two major components: a **score function** that maps the raw data to class scores, and a **loss function** that quantifies the agreement between the predicted scores and the ground truth labels.\n",
    "\n",
    "For example, in CIFAR-10 we have a training set of N = 50,000 images, each with D = 32 x 32 x 3 = 3072 pixels, and K = 10, since there are 10 distinct classes (dog, cat, car, etc). \n",
    "\n",
    "### Linear Classifier\n",
    "\n",
    "$f(x_i, W, b) = Wx_i+b$\n",
    "\n",
    "where $x_i$ contains all pixels in the i-th image flattened into a single [3072 x 1] column. W is often called weights and is [10 x 3072] and b is called bias vector, with size of [10 x 1].\n",
    "\n",
    "**Example**\n",
    "\n",
    "![alt text](./img/classifier.png \"Title\")\n",
    "\n",
    "In the example shown above, the linear classifier compute the scores of a class as a weighted sum of all its pixel values across all 3 of its color channels. We assume the image only has 4 pixels and that we have 3 classes.\n",
    "\n",
    "\n",
    "**Bias Trick**\n",
    "\n",
    "We can combine two sets of parameters (the biases b and weights W) into a single matrix that holds both of them, in which we get:\n",
    "\n",
    "$f(x_i, W) = Wx_i$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function\n",
    "\n",
    "**Multiclass Support Vector Machine (SVM)**\n",
    "\n",
    "SVM loss is setup so that the correct class for each image to have a score higher than the incorrect class by some fixed amount margin $\\Delta$. Notice that it’s sometimes helpful to anthropomorphise the loss functions as we did above: The SVM “wants” a certain outcome in the sense that the outcome would yield a lower loss (which is good).\n",
    "\n",
    "$$L_i = \\sum_{j \\neq y_i}\\max (0, s_j-s_{yi}+\\Delta)$$\n",
    "\n",
    "where $y_i$ is the index of the correct class and $s_j = f(x_i,W)_j$ (the score for j-th class is the j-th element). \n",
    "\n",
    "We can also rewrite the loss function as:\n",
    "\n",
    "$$L_i = \\sum_{j \\neq y_i}\\max (0, w_j^T-w_{yi}^T+\\Delta)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization\n",
    "With the loss function presented above, a potential problem would be that there are a set of parameters W correctly classify every example. We want to encode some preference for certain set of weights W over others to remove this ambiguity.\n",
    "\n",
    "We can do so by extending the loss function with a regularization penalty R(w). The most common regularization penalty is L2 norm.\n",
    "\n",
    "$$R(W) = \\sum_{k} \\sum_{l} W_{k,l}^2$$\n",
    "\n",
    "The full multiclass SVM loss becomes:\n",
    "$$L = \\frac{1}{N} \\sum_i L_i + \\lambda R(W)$$\n",
    "\n",
    "or expanding this out in its full form:\n",
    "\n",
    "$$L = \\frac{1}{N} \\sum_i \\sum_{j \\neq y_i}max(0, w_j^T-w_{yi}^T+\\Delta) + \\lambda \\sum_{k} \\sum_{l} W_{k,l}^2$$\n",
    "\n",
    "L2 penalty leads to the appealing max margin property in SVM. The most appealing property is that penalizing large weights tends to improve generalization, because it means that no input dimension can have a very large influence on the scores by itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Practical Consideration\n",
    " \n",
    " **Setting Delta**\n",
    " \n",
    " It turns out that this hyperparameter can safely be set to $\\Delta$=1.0\n",
    " \n",
    " **Relation to Binary Support Vector Machine**\n",
    " \n",
    " $$L_i = Cmax(0,1,-y_iw^Tx_i)+R(W)$$\n",
    " \n",
    " where C is a hyperparameter and $y_i \\in \\{-1,1\\}$. This can be regard as a special case when there are only two classes in this SVM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax Classifier\n",
    "\n",
    "It turns out that SVM is one of two commonly seen classifiers. The other popular choice is Softmax classifier.\n",
    "\n",
    "$$L_i = -\\log (\\frac{e^{f_{yi}}}{\\sum_je^{f_j}})$$ or equivalently $$L_i = -f_{yi}+\\log \\sum_je^{f_j}$$\n",
    "\n",
    "It takes a vector of arbitrary real-valued scores and squashes it to vector values between 0 and one that sum to one.\n",
    "\n",
    "**Information theory view**\n",
    "\n",
    "The cross-entropy between a \"true\" distribution p and an estimated distributed q is defined as \n",
    "\n",
    "$$H(p,1)=-\\sum_xp(x)l\\log q(x)$$\n",
    "\n",
    "The softmax classifier is hence minimizing cross-entropy between the estimated class probabilities and the \"true\" distribution.\n",
    "\n",
    "Moreover, since the cross-entropy can be written in terms of entropy and the Kullback-Leibler divergence as $H(p,q)=H(p)+D_{KL}(p||q)$ and the entropy of the data function p is zero, this is also equivalent to minimizing the KL divergence between the two distributions.\n",
    "\n",
    "In other words, the cross-entropy objective wants the predicted distribution to have all of its mass on the correct answer.\n",
    "\n",
    "**Probabilistic interpretation**\n",
    "\n",
    "$$P(y_i|x_i:W)=\\frac{e^{f_{yi}}} {\\sum_j e^{f_j}}$$\n",
    "\n",
    "can be interpreted as the probability assigned to the correct label $y_i$, given the image $x_i$ and parameterized by W.\n",
    "\n",
    "**Practical issues: Numeric stability**\n",
    "\n",
    "When we are writing code for computing the Softmax function in practice, the intermediate term $e^{f_{yi}}$ and $\\sum_j e^{f_j}$ may be very large due to exponential. So it is important to use a normalization trick.\n",
    "\n",
    "$$\\frac{e^{f_{yi}}} {\\sum_j e^{f_j}} = \\frac{Ce^{f_{yi}}} {\\sum_j e^{f_j}}=\\frac{e^{f_{yi}+\\log C}} {\\sum_j e^{{f_j}+\\log C}}$$\n",
    "\n",
    "A common choice for C is to set $\\log C = -\\max_jf_j$.\n",
    "\n",
    "**Possibly confusing**\n",
    "\n",
    "To be precise, the SVM classifier uses the hinge loss, or also sometimes called the max-margin loss. The Softmax classifier uses cross-entropy loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM vs. Softmax\n",
    "\n",
    "In both cases we computer the same score vector f. The difference is in the interpretation of the scores in f: The SVM interprets these as class cores adn its loss function encourage the correct class to have a score higher by a margin than the other class scores.\n",
    "\n",
    "The Softmax classifier instead interprets the score as (unnormalized) log probabilities for each class and then encourages the (normalized) log probability of the correct class to be high. \n",
    "\n",
    "In practices, SVM and Softmax are usually comparable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization: Stochastic Gradient Descent\n",
    "\n",
    "Optimization: the process of finding the set of parameters W that minimize the loss function.\n",
    "\n",
    "Assume a simple dataset that contains three 1-dimensional points and three classes. The full SVM loss (without regulation) becomes:\n",
    "\n",
    "$$L_0 = \\max(0,w_1^Tx_0-w_0^Tx_0+1)+\\max(0,w_2^Tx_0-w_0^Tx_0+1)$$\n",
    "$$L_1 = \\max(0,w_0^Tx_1-w_1^Tx_1+1)+\\max(0,w_2^Tx_1-w_1^Tx_1+1)$$\n",
    "$$L_2 = \\max(0,w_0^Tx_2-w_2^Tx_2+1)+\\max(0,w_1^Tx_2-w_2^Tx_2+1)$$\n",
    "$$L=(L_0+L_1+L2)/3$$\n",
    "\n",
    "![alt text](./img/sum.png \"Title\")\n",
    "\n",
    "The SVM cost funciton is an example of a convex function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization Strategy\n",
    "\n",
    "**Random Search**\n",
    "\n",
    "Not quite a good idea..\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestloss = float(\"inf\") # Python assigns the highest possible float value\n",
    "for num in xrange(1000):\n",
    "  W = np.random.randn(10, 3073) * 0.0001 # generate random parameters\n",
    "  loss = L(X_train, Y_train, W) # get the loss over the entire training set\n",
    "  if loss < bestloss: # keep track of the best solution\n",
    "    bestloss = loss\n",
    "    bestW = W\n",
    "  print 'in attempt %d the loss was %f, best %f' % (num, loss, bestloss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Random Local Search**\n",
    "\n",
    "Extend one foot in a random direction and take a step only if it leads downhill. Concretely, we will start out with a random W, generate random perturbation $\\delta$W to it and if the loss at perturbed W+$\\delta$W is lower, we will perform an update\n",
    "\n",
    "**Following the Gradient**\n",
    "\n",
    "The gradient is a generalization of slope for functions that don't take a signle number but a vector of numbers. Gradient is just a vector of slopes (more commonly referred as derivatives) for each dimension in the input space.\n",
    "\n",
    "The mathematical expression for the derivative of a 1-D function with respect to its input is:\n",
    "\n",
    "$$\\frac{df(x)}{dx}=lim_{h \\to 0} \\frac{f(x+h)-f(x)}{h}$$\n",
    "\n",
    "When the functions of interest take a vector of numbers instead of a single number, we call the derivatives partial derivatives and the gradient is simply the vector or partial derivatives in each dimension."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing the gradient numerically with finite differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_numerical_gradient(f, x):\n",
    "  \"\"\" \n",
    "  a naive implementation of numerical gradient of f at x \n",
    "  - f should be a function that takes a single argument\n",
    "  - x is the point (numpy array) to evaluate the gradient at\n",
    "  \"\"\" \n",
    "\n",
    "  fx = f(x) # evaluate function value at original point\n",
    "  grad = np.zeros(x.shape)\n",
    "  h = 0.00001\n",
    "\n",
    "  # iterate over all indexes in x\n",
    "  it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "  while not it.finished:\n",
    "\n",
    "    # evaluate function at x+h\n",
    "    ix = it.multi_index\n",
    "    old_value = x[ix]\n",
    "    x[ix] = old_value + h # increment by h\n",
    "    fxh = f(x) # evalute f(x + h)\n",
    "    x[ix] = old_value # restore to previous value (very important!)\n",
    "\n",
    "    # compute the partial derivative\n",
    "    grad[ix] = (fxh - fx) / h # the slope\n",
    "    it.iternext() # step to next dimension\n",
    "\n",
    "  return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Effect of step size**\n",
    "\n",
    "The gradient tells us the direction in which the function has the steepest rate of increase, but it does not tell us how far along this direction we should step. As we will see later in the course, choosing the step size (h) will become one of the most important hyperparameter settings in training a neural network. \n",
    "\n",
    "**Computing the gradient analytically with Calculus**\n",
    "\n",
    "The numerical gradient is very simple to compute using the finite difference approximation but the downside is that it is approximate and that is very computationally expensive to compute. The second way to compute the gradient is analytically using Calculus, which allows us to derive a direct formula for the gradient that is also very fast to compute. However, unlike the numerical gradient, it can be more error prone to implement, which is why in practice it is very common to compute the analytic gradient and compare it to the numerical gradient to check correctness of your implementation. This is called **gradient check**.\n",
    "\n",
    "With SVM loss function for a single data point:\n",
    "$$L_i = \\sum_{j \\neq y_i}\\max (0, s_j-s_{yi}+\\Delta)$$\n",
    "\n",
    "We can differentiate the function with respect to the weights.\n",
    "\n",
    "$$\\nabla w_{yi}L_i = - (\\sum_{j\\neq y_i}𝟙(w_j^Tx_i-w_{yi}^Tx_i)+\\Delta > 0))x_i$$\n",
    "\n",
    "𝟙 is the indicator function that is one if the condition inside is true or zero otherwise. \n",
    "\n",
    "For the other rows where $j\\neq y_i$, the gradient is:\n",
    "\n",
    "$$\\nabla w_{j}L_i = - (\\sum_{j\\neq y_i}𝟙(w_j^Tx_i-w_{yi}^Tx_i)+\\Delta > 0))x_i$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent\n",
    "\n",
    "Now that we can compute the gradient of the loss function, the procedure of repeatedly evaluating the gradient and then performing a parameter update is called Gradient Descent.\n",
    "\n",
    "**Mini-batch gradient descent**\n",
    "\n",
    "In large-scale applications, it is wasteful to compute full loss function over the entire datasets in order to perform a single parameter update. Avery common approach to address this challenge is to computer gradient over batches of training data.\n",
    "\n",
    "The extreme case of this is a setting where mini-batch contains only a single example. The process is called **Stochastic Gradient Descent (SGD)**. Even though SGD technically refers to using a single example at a time to evaluate the gradient, you will hear people use the term SGD even when referring to mini-batch gradient descent \n",
    "\n",
    "![alt text](./img/summary.png \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation, Intuitions\n",
    "\n",
    "### Simple expression and interpretation of the gradient\n",
    "\n",
    "$$f(x,y)=xy$$\n",
    "$$\\frac{\\partial f}{\\partial x} = y$$\n",
    "$$\\frac{\\partial f}{\\partial y} = x$$\n",
    "\n",
    "\n",
    "Derivative indicates the rate of change of a function with respect to variable surrounding an infinitesimally small region near a particular point:\n",
    "\n",
    "$$\\frac{df(x)}{dx}=\\lim_{h \\to 0} \\frac{f(x+h)-f(x)}{h}$$\n",
    "\n",
    "The derivative of each variable tells you the sensitivity of the whole expression on its value. If we have $\\frac{\\partial f}{\\partial y} = 4$, we expect that increasing value of y by a some very small amount h would increase the output of the function by 4h.\n",
    "\n",
    "The derivative on each variable tells you the sensitivity of the whole expression on its value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compound expression with chain rule\n",
    "\n",
    "For $f(x,y,z)=(x+y)z$, we can break it down into two expressions: $q=x+y$ and $f=qz$. We know $\\frac{\\partial f}{\\partial q}=z$, $\\frac{\\partial f}{\\partial z}=q$ and q is addition of x and y so $\\frac{\\partial q}{\\partial x}=1$, $\\frac{\\partial q}{\\partial y}=1$. However, we don't necessarily care about q, instead we care about gradient of f with respect to its input x,y,z. The chain rule tells us $\\frac{\\partial f}{\\partial x} = \\frac{\\partial f}{\\partial q} \\frac{\\partial q}{\\partial x}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intuitive understanding of backpropagtaion\n",
    "\n",
    "Backpropagation is a local process. Every gate in a circuit diagram gets some inputs and can right away two things: 1. its output value and 2. the local gradient of its inputs with respect to its output value.\n",
    "\n",
    "It is very cost-inefficient to update every parameters for a single neurons. The way backpropagation works is to collect requests from all last-layer neurons and compute backward for the most optimal parameter values.\n",
    "\n",
    "Good video explanation here https://www.youtube.com/watch?v=Ilg3gGewQ5U."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modularity: Sigmoid example\n",
    "\n",
    "$$ f(w,x)= \\frac{1}{1+e^{-(w_0x_0+w_1x_1+w_2)}}$$\n",
    "\n",
    "The function is made up of multiple gates.\n",
    "\n",
    "$$f(x) = \\frac{1}{x} \\ \\frac{\\partial f}{\\partial x}= -\\frac{1}{x^2}$$\n",
    "$$f_c(x) = c+x  \\ \\frac{\\partial f}{\\partial x}= 1$$\n",
    "$$f(x) = e^x  \\ \\frac{\\partial f}{\\partial x}= e^x$$\n",
    "$$f_a(x) = ax \\ \\frac{\\partial f}{\\partial x}= a$$\n",
    "\n",
    "\n",
    "The function the above operations implement is called sigmoid function \n",
    "$$\\sigma(x) = \\frac{1}{1+e^{-x}}$$\n",
    "$$\\frac{d \\sigma(x)}{dx} = \\frac{e^{-x}}{1+e^{-x}}$$\n",
    "$$=\\left (\\frac{1+e^{-x}-1}{1+e^(-x)} \\right) \\left ( \\frac{1}{1+e^{-x}}\\right) = (1-\\sigma(x))\\sigma(x)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = [2,-3,-3] # assume some random weights and data\n",
    "x = [-1, -2]\n",
    "\n",
    "# forward pass\n",
    "dot = w[0]*x[0] + w[1]*x[1] + w[2]\n",
    "f = 1.0 / (1 + math.exp(-dot)) # sigmoid function\n",
    "\n",
    "# backward pass through the neuron (backpropagation)\n",
    "ddot = (1 - f) * f # gradient on dot variable, using the sigmoid gradient derivation\n",
    "dx = [w[0] * ddot, w[1] * ddot] # backprop into x\n",
    "dw = [x[0] * ddot, x[1] * ddot, 1.0 * ddot] # backprop into w\n",
    "# we're done! we have the gradients on the inputs to the circuit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backprop in practice: Staged computation\n",
    "\n",
    "$$f(x,y)=\\frac{x+\\sigma(y)}{\\sigma(x)+(x+y)^2}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward pass\n",
    "x = 3 # example values\n",
    "y = -4\n",
    "\n",
    "# forward pass\n",
    "sigy = 1.0 / (1 + math.exp(-y)) # sigmoid in numerator   #(1)\n",
    "num = x + sigy # numerator                               #(2)\n",
    "sigx = 1.0 / (1 + math.exp(-x)) # sigmoid in denominator #(3)\n",
    "xpy = x + y                                              #(4)\n",
    "xpysqr = xpy**2                                          #(5)\n",
    "den = sigx + xpysqr # denominator                        #(6)\n",
    "invden = 1.0 / den                                       #(7)\n",
    "f = num * invden # done!                                 #(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We structure the in such way that it contains multiple intermediate variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# backprop f = num * invden\n",
    "dnum = invden # gradient on numerator                             #(8)\n",
    "dinvden = num                                                     #(8)\n",
    "# backprop invden = 1.0 / den \n",
    "dden = (-1.0 / (den**2)) * dinvden                                #(7)\n",
    "# backprop den = sigx + xpysqr\n",
    "dsigx = (1) * dden                                                #(6)\n",
    "dxpysqr = (1) * dden                                              #(6)\n",
    "# backprop xpysqr = xpy**2\n",
    "dxpy = (2 * xpy) * dxpysqr                                        #(5)\n",
    "# backprop xpy = x + y\n",
    "dx = (1) * dxpy                                                   #(4)\n",
    "dy = (1) * dxpy                                                   #(4)\n",
    "# backprop sigx = 1.0 / (1 + math.exp(-x))\n",
    "dx += ((1 - sigx) * sigx) * dsigx # Notice += !! See notes below  #(3)\n",
    "# backprop num = x + sigy\n",
    "dx += (1) * dnum                                                  #(2)\n",
    "dsigy = (1) * dnum                                                #(2)\n",
    "# backprop sigy = 1.0 / (1 + math.exp(-y))\n",
    "dy += ((1 - sigy) * sigy) * dsigy                                 #(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks Part 1: Setting up the Architecture\n",
    "\n",
    "### Modeling one neuron\n",
    "\n",
    "**Biological motivation and connections**\n",
    "\n",
    "We model the firing state of the neuron with an activation function f, which represents the frequency of the spikes along the axon.\n",
    "\n",
    "### Single neuron as a linear classifier\n",
    "**Binary Softmax classifier**\n",
    "\n",
    "For example, we can interpret $\\sigma(\\sum_{i}{w_ix_i+b})$ to be the probability of one of the classes $P(y_i-1|x_i;w)$. The probability of the other class would be $P(y_i=0|x_i;w)=1-P(y=1|x_i;w)$.\n",
    "\n",
    "**Binary SVM classifier** \n",
    "Alternatively, we could attach a max-margin hinge loss to output of the neuron and train it to become a binary Support Machine.\n",
    "\n",
    "**Regularization interpretation**\n",
    "The regularization loss in both SVM/Softmax cases could in this biological view be interpreted as gradual forgetting, since it would have the effect of driving all synaptic weights w towards zero after every parameter update."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Commonly used activation functions\n",
    "\n",
    "![alt text](./img/sigmoid.png \"Title\")\n",
    "**Sigmoid**\n",
    "\n",
    "It takes a real-valued number and \"squashes\" into range between 0 and 1. In particular, large negative numbers become 0 and large positive numbers become 1.\n",
    "\n",
    "$$\\sigma (x) = \\frac{1}{1+e^(-x)}$$\n",
    "\n",
    "Drawbacks:\n",
    "\n",
    "* **Sigmoid saturate and kill gradients.** A very undesirable property of the sigmoid neuron is that when activation saturates at either tail of 0 or 1, the gradient at these regions is almost zero. Recall that during backpropagation, this (local) gradient will be multiplied to gradient of this gate's output for the whole objective. Therefore, if the local gradient is very small, it will effectively \"kill\" the gradient and almost no signal will flow through the neuron.\n",
    "\n",
    "* **Sigmoid outputs are not zero-centered.**  This has implications on the dynamics during gradient descent, because if the data coming to a neuron is always positive, then the gradient on the weights w will during backpropagation become either all positive, or all negative.\n",
    "\n",
    "**Tanh** \n",
    "\n",
    "It squashes a real-valued number to rang [-1,1]. Like the sigmoid neuron, its activation saturate, but unlike the sigmoid neuron, its output is zero-centered.\n",
    "\n",
    "$$tanh(x)=2\\sigma(2x)-1$$\n",
    "\n",
    "![alt text](./img/relu.png \"Title\")\n",
    "\n",
    "**ReLU**\n",
    "\n",
    "$$f(x)=max(0,1)$$\n",
    "\n",
    "Pros:\n",
    "* It was found to greatly accelerate the convergence of gradient descent compared to the sigmoid/tanh functions. It is argued that this is due to its linear, non-saturating form.\n",
    "* Compared to tanh/sigmoid neurons that involve expensive operations, the ReLU can be implemented by simply thresholding matrix of activations at zero.\n",
    "\n",
    "Cons:\n",
    "* ReLU units can be fragile during training and can \"die\". \n",
    "\n",
    "**Leaky ReLU**\n",
    "\n",
    "Leaky ReLUs are one attempt to fix the \"dying ReLU\" problem. Instead of the function being zero when x < 0, a leaky ReLU will instead have a small negative slope.\n",
    "\n",
    "The function computes:\n",
    "$$f(x)=1(x<0)(\\alpha x)+1(x>=0)(x)$$\n",
    "\n",
    "where $\\alpha$ is a small constant. \n",
    "\n",
    "\n",
    "**Maxout**\n",
    "\n",
    "Other types of units have been proposed that do not have functional form $f(w^Tx+b)$ where a non-linearity is applied on the dot product between the weights and the data. The Maxout neuron compute the function $max(w_1^Tx+b_1, w_2^Tx+b_2)$. The maxout neuron there enjoys all benefits of a ReLU unit and does not have its drawback (dying ReLU). However, unlike the RelU neuron it doubles the number of parameters for every single neuron, leaning a high total number parameters.\n",
    "\n",
    "**What neuron type should I use?**\n",
    "\n",
    "Use the ReLU non-linearity, be careful with your learning rates and possibly monitor the fraction of \"dead\" units in a network. If this concerns you, give Leaky ReLU or Maxout a try. Never use sigmoid. Try tanh, but expect it to work work worse than ReLU/Maxout."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network architectures\n",
    "\n",
    "**Layer-wise organization**\n",
    "\n",
    "Neural Networks as neurons in graphs. For regular neural networks, the most common layer type is the fully-connected layer in which neurons between two adjacent layers are fully pairwise connected, but neuron within a single layer share no connections.\n",
    "\n",
    "**Naming conventions**\n",
    "Notice that when we say N-layer neural network, we do not count the input layer. \n",
    "\n",
    "**Output layer** \n",
    "Unlike all layers in a Neural Network, the output layer neurons most commonly do not have an activation function.\n",
    "\n",
    "![alt text](./img/nn.png \"Title\")\n",
    "\n",
    "**Sizing neural networks**\n",
    "\n",
    "The left network has $4+2=6$ neurons, $[3x4]+[4x2]=20$ weights and $4+2=6$ biases, for a total 26 learnable parameters.\n",
    "\n",
    "**Representational power**\n",
    "\n",
    "Neural Networks with at least one hidden layer are universal approximators. The fact that deeper networks can work better than single-hidden-layer networks is an empirical observation, despite the fact that their representational power is equal.\n",
    "\n",
    "**Setting number of layers and their sizes**\n",
    "\n",
    "\n",
    "Neural Networks with more neurons can express more complicated functions. Overfitting occurs when a model with high capacity fits the noise in the data instead underlying relationship. Regulation strength is the preferred way to control overfitting of a neural network. S\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
